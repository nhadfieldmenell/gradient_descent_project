Noah Hadfield-Menell

For my project, I will do an comprehensive study of different accelerated gradient descent methods. I will research the specific methods to learn the conditions under which they are advantageous. I will investigate the following optimized methods {Momentum, Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam} and potentially others that I find in my research.  In order to better understand the algorithms, I will implement them manually and apply them to LSR, logistic regression, and Lasso problems.  I will compare their runtimes and number of iterations.  I also will look into implementing frameworks for parameter tuning potentially using backtracking or line search.  Finally, I will implement a multilayer neural network in TensorFlow and compare the runtimes of their built-in implementations of the different optimizers.  My neural net will classify images from the MNIST dataset. 